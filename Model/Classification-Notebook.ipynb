{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "In this project, main libraries we used are:\n",
    "* Tensorflow \n",
    "* Numpy\n",
    "* os\n",
    "* PIL\n",
    "* Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing\n",
    "Here we resized all of the image in a folder to 200x200 pixels. \n",
    "Not that we did this per folder (train and validation), ensuring every image is intact and not corrupted. \n",
    "\n",
    "Our advice is to check every folder to see if there are some corrupted images after resizing, then delete and/or change it immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "f = r'C:\\Users\\Asus\\Downloads\\Compressed\\NEW-BATIK-DATASET\\prg'\n",
    "    \n",
    "new_d = 200\n",
    "\n",
    "for file in os.listdir(f):\n",
    "    f_img = f+'/'+file\n",
    "    try:\n",
    "        img = Image.open(f_img)\n",
    "        img = img.resize((new_d, new_d))\n",
    "        img.save(f_img)\n",
    "    except IOError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image Sources\n",
    "Here we are using **ImageDataGenerator** as a loader for images in our directory.\n",
    "\n",
    "We implemented Image augmentation in the training directory to prevent overfitting. We also load data locally, since we were using Jupyter Notebook with Tensorflow 2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training \n",
    "TRAINING_DIR = \"C:\\\\Users\\\\Asus\\\\Downloads\\\\Compressed\\\\cobatrain\"\n",
    "training_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255,\n",
    "      rotation_range=45,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Load Validation\n",
    "VALIDATION_DIR = \"C:\\\\Users\\\\Asus\\\\Downloads\\\\Compressed\\\\cobaval\"\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(200,200),\n",
    "    class_mode='categorical',\n",
    "    #Expreimented with batch size\n",
    "    batch_size=6)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=(200,200),\n",
    "    class_mode='categorical',\n",
    "    #Same here\n",
    "    batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Keras Model\n",
    "Now that we loaded the images, we create our model using **Convolutional** method.\n",
    "We used 5 Neurons in our final **Dense** layer, because we are identifying for 5 classes.\n",
    "\n",
    "Then, we compile our model with **accuracy** as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(200,200,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Using adam optimizer for faster training\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model\n",
    "Note that we run the model with 200 epochs, we did this because the dataset itself is pretty noisy and requires large epoch for more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    epochs=200,\n",
    "                    # Experiment with steps\n",
    "                    steps_per_epoch=6,\n",
    "                    validation_data = validation_generator,\n",
    "                    shuffle=True,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Graph from history object\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Model\n",
    "And we're done! It's time to check our model.\n",
    "Note that this model expects the input to be 200x200 pixels, and so before importing we need to resize input image first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load dependencies\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load singular image\n",
    "img = image.load_img('C:\\\\Users\\\\Asus\\\\Downloads\\\\motifBatikbaru.jpg', target_size=(200,200))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass image to model.predict\n",
    "images = np.vstack([x])\n",
    "classes = np.argmax(model.predict(images))\n",
    "if classes == [0]:\n",
    "    print(\"Batik Celup\")\n",
    "elif classes == [1]:\n",
    "    print(\"Batik Kawung\")\n",
    "elif classes == [2]:\n",
    "    print(\"Batik Megamendung\")\n",
    "elif classes == [3]:\n",
    "    print(\"Batik Parang\")\n",
    "else:\n",
    "    print(\"Batik Sidoluhur\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "model.save('C:\\\\Users\\\\Asus\\\\Downloads\\\\saved models\\\\savedmodel200v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "As mentioned many, many times, the dataset itself is very noisy. Batik itself is a complex form of art, and as a cultural representative to numerous regions in Indonesia. And so some regions may have very similar pattern even though the name is different.\n",
    "\n",
    "In conclusion, this model can be improved, and the dataset can be improved too. The variety of batik images shouldn't stop our progress to raise awareness for batik types in Indonesia. Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
